{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# !ls ./dataset/bed_pillow_reviews/1-Beckham/\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt') # for sent_tokenize\n",
    "# nltk.download('stopwords') \n",
    "# nltk.download('wordnet') # for WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing/analysis\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from itertools import groupby \n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import io\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "# import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(pathname):\n",
    "    return sorted([os.path.join(pathname, f) for f in os.listdir(pathname)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = './data/bed_pillow_reviews/1-Beckham/'\n",
    "DATA_DIR = './data/bed_pillow_reviews/2-down alt/'\n",
    "all_files = get_paths(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(files, separator=','):\n",
    "    \"\"\"\n",
    "    Takes a list of pathnames and individually reads then concats them into a single DataFrame which is returned.\n",
    "    Can handle Excel files, csv, or delimiter separated text.\n",
    "    \"\"\"\n",
    "    processed_files = []\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.xlsx') or file.lower().endswith('.xls'):\n",
    "            processed_files.append(pd.read_excel(file, index_col=None, header=0))\n",
    "        elif file.lower().endswith('.csv'):\n",
    "            processed_files.append(pd.read_csv(file, index_col=None, header=0))\n",
    "        else:\n",
    "            processed_files.append(pd.read_csv(file, sep=separator, index_col=None, header=0))\n",
    "    completed_df = pd.concat(processed_files, ignore_index=True)\n",
    "    return completed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pillow_reviews = read_files(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeldu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pillow_reviews['ReviewCountry'], pillow_reviews['ReviewDate'] = pillow_reviews['Date'].str.split(' on ', 1).str\n",
    "\n",
    "pillow_reviews['ReviewDate'] = pd.to_datetime(pillow_reviews['ReviewDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews with EITHER Image or Video: 310\n",
      "Reviews with NO Image or Video: 10686\n"
     ]
    }
   ],
   "source": [
    "# Contains Image OR Video\n",
    "print(f\"Reviews with EITHER Image or Video: {len(pillow_reviews[(pillow_reviews.Images!='-') | (pillow_reviews.Videos!='-')])}\")\n",
    "\n",
    "# Contains No Image Nor Video\n",
    "print(f\"Reviews with NO Image or Video: {len(pillow_reviews[(pillow_reviews.Images=='-') & (pillow_reviews.Videos=='-')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT FOR TABLEAU USE\n",
    "# pillow_reviews.to_csv('dataset/pillow_reviews_{}.csv'.format(re.sub(r'(-|:| )', '', str(datetime.now())[:-7])), encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variation</th>\n",
       "      <th>Rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">-</th>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">B01LYC1XSM</th>\n",
       "      <th>1</th>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">B01LYNW421</th>\n",
       "      <th>1</th>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Rating\n",
       "                   count\n",
       "Variation  Rating       \n",
       "-          1          13\n",
       "           2          10\n",
       "           3          12\n",
       "           4          25\n",
       "           5          24\n",
       "B01LYC1XSM 1         582\n",
       "           2         262\n",
       "           3         331\n",
       "           4         392\n",
       "           5        1118\n",
       "B01LYNW421 1        1220\n",
       "           2         735\n",
       "           3         930\n",
       "           4        1484\n",
       "           5        3858"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pillow_reviews.groupby(['Variation', 'Rating']).agg({'Rating': [\"count\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod_1 = pillow_reviews[pillow_reviews['Variation'] == 'B01LYNW421']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_1_good = prod_1[prod_1['Rating'] >= 4]\n",
    "# prod_1_ok = prod_1[prod_1['Rating'] == 3]\n",
    "prod_1_bad = prod_1[prod_1['Rating'] <= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_1_good_reviews = prod_1_good.reset_index()['Body']\n",
    "# prod_1_ok_reviews = prod_1_ok.reset_index()['Body']\n",
    "prod_1_bad_reviews = prod_1_bad.reset_index()['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings_split = combined_strings.split()\n",
    "# freq_splits = FreqDist(strings_split)\n",
    "# print(freq_splits.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short = set(s for s in strings_split if len(s)<4)\n",
    "# short = [(s, freq_splits[s]) for s in short]\n",
    "# short.sort(key=lambda x:x[1], reverse=True)\n",
    "# short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long = set(s for s in strings_split if len(s)>10)\n",
    "# long = [(s, freq_splits[s]) for s in long]\n",
    "# long.sort(key=lambda x:x[1], reverse=True)\n",
    "# long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(pattern, strings, freq):\n",
    "    \"\"\"Summarise strings matching a pattern.\"\"\"\n",
    "    # Find matches\n",
    "    compiled_pattern = re.compile(pattern)\n",
    "    matches = [s for s in strings if compiled_pattern.search(s)]\n",
    "    \n",
    "    # Print volume and proportion of matches\n",
    "    print(\"{} strings, that is {:.2%} of total\".format(len(matches), len(matches)/ len(strings)))\n",
    "    \n",
    "    # Create list of tuples containing matches and their frequency\n",
    "    output = [(s, freq[s]) for s in set(matches)]\n",
    "    output.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise(r\"\\d\", strings_split, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyphenated words\n",
    "# summarise(r\"\\w+-+\\w+\", strings_split, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outlaw(word):\n",
    "    \"\"\"Find words that contain a same character 3+ times in a row.\"\"\"\n",
    "    is_outlaw = False\n",
    "    for i, letter in enumerate(word):\n",
    "        if i > 1:\n",
    "            if word[i] == word[i-1] == word[i-2] and word[i].isalpha():\n",
    "                is_outlaw = True\n",
    "                break\n",
    "    return is_outlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 strings, that is 0.03% of total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sooo', 3),\n",
       " ('Sooooo', 2),\n",
       " ('soooo', 2),\n",
       " ('Waaaay', 2),\n",
       " ('ahhhh', 1),\n",
       " ('ARRRGH,', 1),\n",
       " ('beautifullll', 1),\n",
       " ('theee', 1),\n",
       " ('suppportive', 1),\n",
       " ('PAAAIIINN!!!\"But', 1),\n",
       " ('louddd.', 1),\n",
       " ('ugghhh', 1),\n",
       " ('waaaaaaay', 1),\n",
       " ('soooooo', 1),\n",
       " ('head...sooo', 1),\n",
       " ('waaaay', 1),\n",
       " ('teeeeeny', 1),\n",
       " ('goood', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings_split = \" \".join(prod_1_bad_reviews).split()\n",
    "outlaws = [s for s in strings_split if find_outlaw(s)]\n",
    "print(\"{} strings, that is {:.2%} of total\".format(len(outlaws), len(outlaws)/ len(strings_split)))\n",
    "freq_splits = FreqDist(strings_split)\n",
    "outlaw_freq = [(s, freq_splits[s]) for s in set(outlaws)]\n",
    "outlaw_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "outlaw_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(reviews):\n",
    "    \"\"\"Takes a list of documents and joins them together, before creating a giant lemmatized list of tokens\"\"\"\n",
    "    combined_strings = \" \".join(list(reviews))\n",
    "\n",
    "    tokeniser = RegexpTokenizer(\"[A-Za-z\\']+|\\.\")\n",
    "    tokens = tokeniser.tokenize(combined_strings)\n",
    "    \n",
    "    # Remove repeat\n",
    "    deduped_tokens = [i[0] for i in groupby(tokens)]\n",
    "    \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    tokens_norm = [lemmatiser.lemmatize(t.lower(), \"v\") for t in deduped_tokens]\n",
    "    \n",
    "    return tokens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tree(tokens):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a function that takes an optional \n",
    "    string which will be searched for for particular n-grams\n",
    "    \"\"\"\n",
    "\n",
    "    #remove all grams starting with period\n",
    "    def _clean_grams(ngram_counter):\n",
    "        begins_with_period = []\n",
    "        for gram in ngram_counter:\n",
    "            if gram[0] == '.':\n",
    "                begins_with_period.append(gram)\n",
    "        for gram in begins_with_period:\n",
    "            del ngram_counter[gram]\n",
    "\n",
    "    def _word_tree(head=None, show_count=20, trailing=2, direction='forward', levels=0, indent=0):\n",
    "        if type(head)==str:\n",
    "            head = head.lower().split()\n",
    "        trailing_grams = trailing \n",
    "        if head != None:\n",
    "            trailing_grams += len(head)\n",
    "        if head==None:\n",
    "            ngram_counter = Counter(ngrams(tokens, trailing_grams))\n",
    "        else:\n",
    "            if direction == 'forward':\n",
    "                ngram_counter = Counter([gram for gram in ngrams(tokens, trailing_grams) if gram[:len(head)] == tuple(head)])\n",
    "            elif direction == 'backward':\n",
    "                ngram_counter = Counter([gram for gram in ngrams(tokens, trailing_grams) if gram[-len(head):] == tuple(head)])\n",
    "            else:\n",
    "                ngram_counter = Counter([gram for gram in ngrams(tokens, trailing_grams) if gram[:len(head)] == tuple(head)])\n",
    "\n",
    "        _clean_grams(ngram_counter)\n",
    "        \n",
    "        for (text, idx) in ngram_counter.most_common(show_count):\n",
    "            print(f\"{'  '*indent}{idx} - {' '.join(text)}\")\n",
    "            if levels > 0 and idx > 3:\n",
    "                _word_tree(text, show_count=3, trailing=2, direction=direction, levels=levels-1, indent=indent+1)\n",
    "\n",
    "    return _word_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod_1_bad_reviews\n",
    "# prod_1_good_reviews\n",
    "\n",
    "TEXT_TO_ANALYZE = prod_1_bad_reviews\n",
    "\n",
    "word_tree = get_word_tree(generate_token(TEXT_TO_ANALYZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 - no support at all\n",
      "  9 - no support at all . i\n",
      "    1 - no support at all . i like the\n",
      "    1 - no support at all . i m a\n",
      "    1 - no support at all . i be so\n",
      "  2 - no support at all . flat\n",
      "  2 - no support at all . one\n",
      "46 - i wake up with\n",
      "  11 - i wake up with neck pain\n",
      "    4 - i wake up with neck pain . i\n",
      "      1 - i wake up with neck pain . i figure it\n",
      "      1 - i wake up with neck pain . i buy these\n",
      "      1 - i wake up with neck pain . i gotta return\n",
      "    1 - i wake up with neck pain . the\n",
      "    1 - i wake up with neck pain the next\n",
      "  4 - i wake up with a sore\n",
      "    1 - i wake up with a sore neck because\n",
      "    1 - i wake up with a sore back good\n",
      "    1 - i wake up with a sore neck and\n",
      "  4 - i wake up with a stiff\n",
      "    2 - i wake up with a stiff neck .\n",
      "    1 - i wake up with a stiff neck when\n",
      "    1 - i wake up with a stiff neck what\n",
      "40 - i be look for\n",
      "  4 - i be look for . i\n",
      "    1 - i be look for . i feel like\n",
      "    1 - i be look for . i feel the\n",
      "    1 - i be look for . i purchase this\n",
      "  3 - i be look for a pillow\n",
      "  3 - i be look for at all\n",
      "38 - support at all .\n",
      "  2 - support at all . i m\n",
      "  2 - support at all . i be\n",
      "  2 - support at all . if you\n",
      "38 - at all . i\n",
      "  2 - at all . i wake up\n",
      "  2 - at all . i buy these\n",
      "  2 - at all . i don t\n",
      "35 - you lay your head\n",
      "  11 - you lay your head on them\n",
      "    2 - you lay your head on them . the\n",
      "    1 - you lay your head on them they sink\n",
      "    1 - you lay your head on them then they\n",
      "  6 - you lay your head on it\n",
      "    1 - you lay your head on it just flatten\n",
      "    1 - you lay your head on it and your\n",
      "    1 - you lay your head on it . i\n",
      "  3 - you lay your head down .\n",
      "34 - a side sleeper and\n",
      "  4 - a side sleeper and these pillow\n",
      "    1 - a side sleeper and these pillow don t\n",
      "    1 - a side sleeper and these pillow have insane\n",
      "    1 - a side sleeper and these pillow cause some\n",
      "  2 - a side sleeper and the pillow\n",
      "  2 - a side sleeper and i be\n",
      "33 - wake up with a\n",
      "  5 - wake up with a stiff neck\n",
      "    1 - wake up with a stiff neck since use\n",
      "    1 - wake up with a stiff neck when i\n",
      "    1 - wake up with a stiff neck . they\n",
      "  4 - wake up with a sore neck\n",
      "    1 - wake up with a sore neck because it\n",
      "    1 - wake up with a sore neck and headaches\n",
      "    1 - wake up with a sore neck and a\n",
      "  2 - wake up with a neck ache\n",
      "32 - i buy these pillow\n",
      "  3 - i buy these pillow because they\n",
      "  2 - i buy these pillow because of\n",
      "  2 - i buy these pillow last year\n",
      "31 - for me . i\n",
      "  3 - for me . i buy these\n",
      "  1 - for me . i just get\n",
      "  1 - for me . i have have\n",
      "31 - be a side sleeper\n",
      "  4 - be a side sleeper and i\n",
      "    1 - be a side sleeper and i think i\n",
      "    1 - be a side sleeper and i be hop\n",
      "    1 - be a side sleeper and i be hopeful\n",
      "  2 - be a side sleeper and the\n",
      "  2 - be a side sleeper and these\n",
      "30 - be not the same\n",
      "  4 - be not the same as the\n",
      "    1 - be not the same as the first set\n",
      "    1 - be not the same as the previous ones\n",
      "    1 - be not the same as the product that\n",
      "  4 - be not the same . they\n",
      "    1 - be not the same . they seem to\n",
      "    1 - be not the same . they look nice\n",
      "    1 - be not the same . they be under\n",
      "  2 - be not the same at all\n",
      "30 - pillow . they be\n",
      "  2 - pillow . they be soft and\n",
      "  1 - pillow . they be really good\n",
      "  1 - pillow . they be just standard\n",
      "29 - wake up with neck\n",
      "  8 - wake up with neck pain .\n",
      "    1 - wake up with neck pain . i figure\n",
      "    1 - wake up with neck pain . the pillow\n",
      "    1 - wake up with neck pain . i buy\n",
      "  5 - wake up with neck pain and\n",
      "    1 - wake up with neck pain and headache with\n",
      "    1 - wake up with neck pain and have trouble\n",
      "    1 - wake up with neck pain and a migraine\n",
      "  2 - wake up with neck pain the\n",
      "28 - these pillow be not\n",
      "  2 - these pillow be not cool .\n",
      "  2 - these pillow be not gel pillow\n",
      "  2 - these pillow be not what i\n"
     ]
    }
   ],
   "source": [
    "word_tree(None, show_count=15, trailing=4, direction='forward', levels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 - i be look for . i\n",
      "  1 - i be look for . i feel like\n",
      "  1 - i be look for . i feel the\n",
      "  1 - i be look for . i purchase this\n",
      "3 - i be look for a pillow\n",
      "3 - i be look for at all\n",
      "2 - i be look for pillow that\n",
      "1 - i be look for that hotel\n",
      "1 - i be look for a proper\n",
      "1 - i be look for at half\n",
      "1 - i be look for cool and\n",
      "1 - i be look for a fluffy\n",
      "1 - i be look for a posture\n"
     ]
    }
   ],
   "source": [
    "word_tree('i be look for', show_count=10, trailing=2, direction='forward', levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 - be not what i be look for\n",
      "1 - and fluffy but i be look for\n",
      "1 - side sleeper so i be look for\n",
      "1 - choice hotel pillow i be look for\n",
      "1 - extra support . i be look for\n",
      "1 - it be but i be look for\n",
      "1 - for sleep . i be look for\n",
      "1 - primary pillow . i be look for\n",
      "1 - they be heavy i be look for\n",
      "1 - not the support i be look for\n"
     ]
    }
   ],
   "source": [
    "word_tree('i be look for', 10, 3, 'backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = stopwords.words(\"english\")\n",
    "# print(f\"There are {len(stop_words)} stopwords.\\n\")\n",
    "# print(stop_words)\n",
    "# my_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "#                 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "#                 \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "#                 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "#                 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "#                 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "#                 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 's', 't', 'can', 'will', 'just', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from credentials import ACCESS_KEY, SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_review = pillow_reviews.Body[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comprehend = boto3.client(\n",
    "#     service_name='comprehend', \n",
    "#     region_name='us-west-2',\n",
    "#     aws_access_key_id=ACCESS_KEY,\n",
    "#     aws_secret_access_key=SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectKeyPhrases\n",
      "{\n",
      "    \"KeyPhrases\": [\n",
      "        {\n",
      "            \"BeginOffset\": 19,\n",
      "            \"EndOffset\": 32,\n",
      "            \"Score\": 0.9999992251396179,\n",
      "            \"Text\": \"these pillows\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 43,\n",
      "            \"EndOffset\": 60,\n",
      "            \"Score\": 1.0,\n",
      "            \"Text\": \"the perfect combo\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 124,\n",
      "            \"EndOffset\": 137,\n",
      "            \"Score\": 1.0,\n",
      "            \"Text\": \"less soreness\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 141,\n",
      "            \"EndOffset\": 148,\n",
      "            \"Score\": 0.9999998211860657,\n",
      "            \"Text\": \"my neck\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 182,\n",
      "            \"EndOffset\": 202,\n",
      "            \"Score\": 0.9999993443489075,\n",
      "            \"Text\": \"a memory foam pillow\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 274,\n",
      "            \"EndOffset\": 287,\n",
      "            \"Score\": 0.9999448657035828,\n",
      "            \"Text\": \"these pillows\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 301,\n",
      "            \"EndOffset\": 320,\n",
      "            \"Score\": 0.999999463558197,\n",
      "            \"Text\": \"a vacuum-sealed bag\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 375,\n",
      "            \"EndOffset\": 390,\n",
      "            \"Score\": 0.9998744130134583,\n",
      "            \"Text\": \"no mold problem\"\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"682\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Sat, 08 May 2021 07:34:00 GMT\",\n",
      "            \"x-amzn-requestid\": \"b2063ecc-0dae-4135-b673-f984b6d181f5\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"b2063ecc-0dae-4135-b673-f984b6d181f5\",\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "End of DetectKeyPhrases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text = single_review\n",
    "\n",
    "# print('Calling DetectKeyPhrases')\n",
    "# print(json.dumps(comprehend.detect_key_phrases(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "# print('End of DetectKeyPhrases\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
